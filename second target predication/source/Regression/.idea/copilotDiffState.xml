<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/DataCleaner.ipynb">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/DataCleaner.ipynb" />
              <option name="originalContent" value="#%%&#10;import pandas as pd&#10;import numpy as np&#10;import matplotlib.pyplot as plt&#10;import seaborn as sns&#10;from sklearn.model_selection import train_test_split&#10;from sklearn.preprocessing import StandardScaler, OneHotEncoder&#10;from sklearn.compose import ColumnTransformer&#10;from sklearn.pipeline import Pipeline&#10;&#10;#%%&#10;cardioRaw=pd.read_parquet('./cardio.parquet')&#10;#%% md&#10;Age | Objective Feature | age | int (days)&#10;&#10;Height | Objective Feature | height | int (cm) |&#10;&#10;Weight | Objective Feature | weight | float (kg) |&#10;&#10;Gender | Objective Feature | gender | categorical code |&#10;&#10;Systolic blood pressure | Examination Feature | ap_hi | int |&#10;&#10;Diastolic blood pressure | Examination Feature | ap_lo | int |&#10;&#10;Cholesterol | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal |&#10;&#10;Glucose | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal |&#10;&#10;Smoking | Subjective Feature | smoke | binary |&#10;&#10;Alcohol intake | Subjective Feature | alco | binary |&#10;&#10;Physical activity | Subjective Feature | active | binary |&#10;&#10;Presence or absence of cardiovascular disease | Target Variable | cardio | binary |&#10;&#10;#%%&#10;# Explore dataset structure&#10;print(&quot;Dataset shape:&quot;, cardioRaw.shape)&#10;cardioRaw.head()&#10;&#10;#%%&#10;# Check for missing values&#10;print(&quot;Missing values:&quot;)&#10;print(cardioRaw.isnull().sum())&#10;&#10;#%%&#10;# Check data types&#10;print(&quot;Data types:&quot;)&#10;print(cardioRaw.dtypes)&#10;&#10;#%%&#10;# Basic statistics&#10;cardioRaw.describe()&#10;&#10;#%%&#10;# Check the distribution of numerical features&#10;numerical_features = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']&#10;&#10;fig, axes = plt.subplots(2, 3, figsize=(15, 10))&#10;axes = axes.ravel()&#10;&#10;for i, feature in enumerate(numerical_features):&#10;    axes[i].hist(cardioRaw[feature], bins=50, alpha=0.7)&#10;    axes[i].set_title(f'Distribution of {feature}')&#10;    axes[i].set_xlabel(feature)&#10;    axes[i].set_ylabel('Frequency')&#10;&#10;# Remove the empty subplot&#10;axes[5].remove()&#10;plt.tight_layout()&#10;plt.show()&#10;&#10;#%%&#10;# Analyze correlations between numerical features&#10;correlation_matrix = cardioRaw[numerical_features].corr()&#10;plt.figure(figsize=(10, 8))&#10;sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)&#10;plt.title('Correlation Matrix of Numerical Features')&#10;plt.show()&#10;&#10;#%%&#10;# Check for outliers in numerical features&#10;plt.figure(figsize=(15, 5))&#10;for i, feature in enumerate(numerical_features):&#10;    plt.subplot(1, 5, i+1)&#10;    plt.boxplot(cardioRaw[feature])&#10;    plt.title(f'{feature}')&#10;    plt.xticks([])&#10;plt.tight_layout()&#10;plt.show()&#10;&#10;#%%&#10;# Calculate BMI as a potential derived numerical target&#10;cardioRaw['bmi'] = cardioRaw['weight'] / ((cardioRaw['height'] / 100) ** 2)&#10;&#10;print(&quot;BMI Analysis:&quot;)&#10;print(f&quot;BMI Range: {cardioRaw['bmi'].min():.2f} - {cardioRaw['bmi'].max():.2f}&quot;)&#10;print(f&quot;BMI Mean: {cardioRaw['bmi'].mean():.2f}&quot;)&#10;print(f&quot;BMI Std: {cardioRaw['bmi'].std():.2f}&quot;)&#10;&#10;# BMI categories&#10;print(&quot;\nBMI Categories:&quot;)&#10;print(f&quot;Underweight (&lt;18.5): {(cardioRaw['bmi'] &lt; 18.5).sum()}&quot;)&#10;print(f&quot;Normal (18.5-24.9): {((cardioRaw['bmi'] &gt;= 18.5) &amp; (cardioRaw['bmi'] &lt; 25)).sum()}&quot;)&#10;print(f&quot;Overweight (25-29.9): {((cardioRaw['bmi'] &gt;= 25) &amp; (cardioRaw['bmi'] &lt; 30)).sum()}&quot;)&#10;print(f&quot;Obese (≥30): {(cardioRaw['bmi'] &gt;= 30).sum()}&quot;)&#10;&#10;#%%&#10;# Visualize BMI distribution&#10;plt.figure(figsize=(12, 4))&#10;&#10;plt.subplot(1, 2, 1)&#10;plt.hist(cardioRaw['bmi'], bins=50, alpha=0.7, color='skyblue')&#10;plt.title('BMI Distribution')&#10;plt.xlabel('BMI')&#10;plt.ylabel('Frequency')&#10;&#10;plt.subplot(1, 2, 2)&#10;plt.boxplot(cardioRaw['bmi'])&#10;plt.title('BMI Boxplot')&#10;plt.ylabel('BMI')&#10;&#10;plt.tight_layout()&#10;plt.show()&#10;&#10;#%%&#10;# Recommended numerical target variable selection&#10;print(&quot;=&quot;*60)&#10;print(&quot;RECOMMENDATION FOR NUMERICAL TARGET VARIABLE&quot;)&#10;print(&quot;=&quot;*60)&#10;print(&quot;\nBased on the analysis, here are the best options for numerical target variables:&quot;)&#10;print(&quot;\n1. SYSTOLIC BLOOD PRESSURE (ap_hi) - RECOMMENDED&quot;)&#10;print(&quot;   ✓ Continuous numerical variable&quot;)&#10;print(&quot;   ✓ Clinical significance&quot;)&#10;print(&quot;   ✓ Good range of values for regression&quot;)&#10;print(&quot;   ✓ Related to cardiovascular health&quot;)&#10;print(&quot;   ✓ Medically interpretable results&quot;)&#10;&#10;print(&quot;\n2. BMI (derived from height/weight) - ALTERNATIVE&quot;)&#10;print(&quot;   ✓ Continuous numerical variable&quot;)&#10;print(&quot;   ✓ Health-related target&quot;)&#10;print(&quot;   ✓ Well-understood medical metric&quot;)&#10;print(&quot;   ✓ Good for demonstrating feature engineering&quot;)&#10;&#10;print(&quot;\n3. DIASTOLIC BLOOD PRESSURE (ap_lo) - ALTERNATIVE&quot;)&#10;print(&quot;   ✓ Continuous numerical variable&quot;)&#10;print(&quot;   ✓ Clinical significance&quot;)&#10;print(&quot;   ✓ Complementary to systolic pressure&quot;)&#10;&#10;print(&quot;\n&quot; + &quot;=&quot;*60)&#10;print(&quot;SELECTED TARGET: SYSTOLIC BLOOD PRESSURE (ap_hi)&quot;)&#10;print(&quot;=&quot;*60)&#10;&#10;#%%&#10;# Data cleaning for the selected target variable (ap_hi)&#10;print(&quot;Data cleaning for systolic blood pressure (ap_hi):&quot;)&#10;&#10;# Check for unrealistic values&#10;print(f&quot;Original data points: {len(cardioRaw)}&quot;)&#10;print(f&quot;ap_hi range: {cardioRaw['ap_hi'].min()} - {cardioRaw['ap_hi'].max()}&quot;)&#10;&#10;# Remove unrealistic blood pressure values (outside physiologically possible range)&#10;# Systolic BP should be between 70-250 mmHg for alive individuals&#10;cardio_cleaned = cardioRaw[(cardioRaw['ap_hi'] &gt;= 70) &amp; (cardioRaw['ap_hi'] &lt;= 250)].copy()&#10;print(f&quot;After removing unrealistic ap_hi values: {len(cardio_cleaned)}&quot;)&#10;&#10;# Also clean diastolic pressure for consistency&#10;cardio_cleaned = cardio_cleaned[(cardio_cleaned['ap_lo'] &gt;= 40) &amp; (cardio_cleaned['ap_lo'] &lt;= 150)]&#10;print(f&quot;After removing unrealistic ap_lo values: {len(cardio_cleaned)}&quot;)&#10;&#10;# Ensure systolic &gt; diastolic&#10;cardio_cleaned = cardio_cleaned[cardio_cleaned['ap_hi'] &gt; cardio_cleaned['ap_lo']]&#10;print(f&quot;After ensuring ap_hi &gt; ap_lo: {len(cardio_cleaned)}&quot;)&#10;&#10;print(f&quot;\nFinal cleaned dataset size: {len(cardio_cleaned)}&quot;)&#10;print(f&quot;Removed {len(cardioRaw) - len(cardio_cleaned)} rows ({((len(cardioRaw) - len(cardio_cleaned))/len(cardioRaw)*100):.2f}%)&quot;)&#10;&#10;#%%&#10;# Final dataset summary with ap_hi as target&#10;print(&quot;FINAL DATASET SUMMARY&quot;)&#10;print(&quot;=&quot;*40)&#10;print(f&quot;Target Variable: ap_hi (Systolic Blood Pressure)&quot;)&#10;print(f&quot;Dataset size: {len(cardio_cleaned)} samples&quot;)&#10;print(f&quot;Features: {len(cardio_cleaned.columns) - 1}&quot;)&#10;print(f&quot;Target range: {cardio_cleaned['ap_hi'].min()} - {cardio_cleaned['ap_hi'].max()}&quot;)&#10;print(f&quot;Target mean: {cardio_cleaned['ap_hi'].mean():.2f}&quot;)&#10;print(f&quot;Target std: {cardio_cleaned['ap_hi'].std():.2f}&quot;)&#10;&#10;# Display final cleaned dataset&#10;cardio_cleaned.head()&#10;&#10;#%%&#10;" />
              <option name="updatedContent" value="#%%&#10;import pandas as pd&#10;import numpy as np&#10;import matplotlib.pyplot as plt&#10;import seaborn as sns&#10;from sklearn.model_selection import train_test_split&#10;from sklearn.preprocessing import StandardScaler, OneHotEncoder&#10;from sklearn.compose import ColumnTransformer&#10;from sklearn.pipeline import Pipeline&#10;&#10;#%%&#10;cardioRaw=pd.read_parquet('./cardio.parquet')&#10;#%% md&#10;Age | Objective Feature | age | int (days)&#10;&#10;Height | Objective Feature | height | int (cm) |&#10;&#10;Weight | Objective Feature | weight | float (kg) |&#10;&#10;Gender | Objective Feature | gender | categorical code |&#10;&#10;Systolic blood pressure | Examination Feature | ap_hi | int |&#10;&#10;Diastolic blood pressure | Examination Feature | ap_lo | int |&#10;&#10;Cholesterol | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal |&#10;&#10;Glucose | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal |&#10;&#10;Smoking | Subjective Feature | smoke | binary |&#10;&#10;Alcohol intake | Subjective Feature | alco | binary |&#10;&#10;Physical activity | Subjective Feature | active | binary |&#10;&#10;Presence or absence of cardiovascular disease | Target Variable | cardio | binary |&#10;&#10;#%%&#10;# Explore dataset structure&#10;print(&quot;Dataset shape:&quot;, cardioRaw.shape)&#10;cardioRaw.head()&#10;&#10;#%%&#10;# Check for missing values&#10;print(&quot;Missing values:&quot;)&#10;print(cardioRaw.isnull().sum())&#10;&#10;#%%&#10;# Check data types&#10;print(&quot;Data types:&quot;)&#10;print(cardioRaw.dtypes)&#10;&#10;#%%&#10;# Basic statistics&#10;cardioRaw.describe()&#10;&#10;#%%&#10;# Check the distribution of numerical features&#10;numerical_features = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']&#10;&#10;fig, axes = plt.subplots(2, 3, figsize=(15, 10))&#10;axes = axes.ravel()&#10;&#10;for i, feature in enumerate(numerical_features):&#10;    axes[i].hist(cardioRaw[feature], bins=50, alpha=0.7)&#10;    axes[i].set_title(f'Distribution of {feature}')&#10;    axes[i].set_xlabel(feature)&#10;    axes[i].set_ylabel('Frequency')&#10;&#10;# Remove the empty subplot&#10;axes[5].remove()&#10;plt.tight_layout()&#10;plt.show()&#10;&#10;#%%&#10;# Analyze correlations between numerical features&#10;correlation_matrix = cardioRaw[numerical_features].corr()&#10;plt.figure(figsize=(10, 8))&#10;sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)&#10;plt.title('Correlation Matrix of Numerical Features')&#10;plt.show()&#10;&#10;#%%&#10;# Check for outliers in numerical features&#10;plt.figure(figsize=(15, 5))&#10;for i, feature in enumerate(numerical_features):&#10;    plt.subplot(1, 5, i+1)&#10;    plt.boxplot(cardioRaw[feature])&#10;    plt.title(f'{feature}')&#10;    plt.xticks([])&#10;plt.tight_layout()&#10;plt.show()&#10;&#10;#%%&#10;# Calculate BMI as a potential derived numerical target&#10;cardioRaw['bmi'] = cardioRaw['weight'] / ((cardioRaw['height'] / 100) ** 2)&#10;&#10;print(&quot;BMI Analysis:&quot;)&#10;print(f&quot;BMI Range: {cardioRaw['bmi'].min():.2f} - {cardioRaw['bmi'].max():.2f}&quot;)&#10;print(f&quot;BMI Mean: {cardioRaw['bmi'].mean():.2f}&quot;)&#10;print(f&quot;BMI Std: {cardioRaw['bmi'].std():.2f}&quot;)&#10;&#10;# BMI categories&#10;print(&quot;\nBMI Categories:&quot;)&#10;print(f&quot;Underweight (&lt;18.5): {(cardioRaw['bmi'] &lt; 18.5).sum()}&quot;)&#10;print(f&quot;Normal (18.5-24.9): {((cardioRaw['bmi'] &gt;= 18.5) &amp; (cardioRaw['bmi'] &lt; 25)).sum()}&quot;)&#10;print(f&quot;Overweight (25-29.9): {((cardioRaw['bmi'] &gt;= 25) &amp; (cardioRaw['bmi'] &lt; 30)).sum()}&quot;)&#10;print(f&quot;Obese (≥30): {(cardioRaw['bmi'] &gt;= 30).sum()}&quot;)&#10;&#10;#%%&#10;# Visualize BMI distribution&#10;plt.figure(figsize=(12, 4))&#10;&#10;plt.subplot(1, 2, 1)&#10;plt.hist(cardioRaw['bmi'], bins=50, alpha=0.7, color='skyblue')&#10;plt.title('BMI Distribution')&#10;plt.xlabel('BMI')&#10;plt.ylabel('Frequency')&#10;&#10;plt.subplot(1, 2, 2)&#10;plt.boxplot(cardioRaw['bmi'])&#10;plt.title('BMI Boxplot')&#10;plt.ylabel('BMI')&#10;&#10;plt.tight_layout()&#10;plt.show()&#10;&#10;#%%&#10;# Recommended numerical target variable selection&#10;print(&quot;=&quot;*60)&#10;print(&quot;RECOMMENDATION FOR NUMERICAL TARGET VARIABLE&quot;)&#10;print(&quot;=&quot;*60)&#10;print(&quot;\nBased on the analysis, here are the best options for numerical target variables:&quot;)&#10;print(&quot;\n1. SYSTOLIC BLOOD PRESSURE (ap_hi) - RECOMMENDED&quot;)&#10;print(&quot;   ✓ Continuous numerical variable&quot;)&#10;print(&quot;   ✓ Clinical significance&quot;)&#10;print(&quot;   ✓ Good range of values for regression&quot;)&#10;print(&quot;   ✓ Related to cardiovascular health&quot;)&#10;print(&quot;   ✓ Medically interpretable results&quot;)&#10;&#10;print(&quot;\n2. BMI (derived from height/weight) - ALTERNATIVE&quot;)&#10;print(&quot;   ✓ Continuous numerical variable&quot;)&#10;print(&quot;   ✓ Health-related target&quot;)&#10;print(&quot;   ✓ Well-understood medical metric&quot;)&#10;print(&quot;   ✓ Good for demonstrating feature engineering&quot;)&#10;&#10;print(&quot;\n3. DIASTOLIC BLOOD PRESSURE (ap_lo) - ALTERNATIVE&quot;)&#10;print(&quot;   ✓ Continuous numerical variable&quot;)&#10;print(&quot;   ✓ Clinical significance&quot;)&#10;print(&quot;   ✓ Complementary to systolic pressure&quot;)&#10;&#10;print(&quot;\n&quot; + &quot;=&quot;*60)&#10;print(&quot;SELECTED TARGET: SYSTOLIC BLOOD PRESSURE (ap_hi)&quot;)&#10;print(&quot;=&quot;*60)&#10;&#10;#%%&#10;# Data cleaning for the selected target variable (ap_hi)&#10;print(&quot;Data cleaning for systolic blood pressure (ap_hi):&quot;)&#10;&#10;# Check for unrealistic values&#10;print(f&quot;Original data points: {len(cardioRaw)}&quot;)&#10;print(f&quot;ap_hi range: {cardioRaw['ap_hi'].min()} - {cardioRaw['ap_hi'].max()}&quot;)&#10;&#10;# Remove unrealistic blood pressure values (outside physiologically possible range)&#10;# Systolic BP should be between 70-250 mmHg for alive individuals&#10;cardio_cleaned = cardioRaw[(cardioRaw['ap_hi'] &gt;= 70) &amp; (cardioRaw['ap_hi'] &lt;= 250)].copy()&#10;print(f&quot;After removing unrealistic ap_hi values: {len(cardio_cleaned)}&quot;)&#10;&#10;# Also clean diastolic pressure for consistency&#10;cardio_cleaned = cardio_cleaned[(cardio_cleaned['ap_lo'] &gt;= 40) &amp; (cardio_cleaned['ap_lo'] &lt;= 150)]&#10;print(f&quot;After removing unrealistic ap_lo values: {len(cardio_cleaned)}&quot;)&#10;&#10;# Ensure systolic &gt; diastolic&#10;cardio_cleaned = cardio_cleaned[cardio_cleaned['ap_hi'] &gt; cardio_cleaned['ap_lo']]&#10;print(f&quot;After ensuring ap_hi &gt; ap_lo: {len(cardio_cleaned)}&quot;)&#10;&#10;print(f&quot;\nFinal cleaned dataset size: {len(cardio_cleaned)}&quot;)&#10;print(f&quot;Removed {len(cardioRaw) - len(cardio_cleaned)} rows ({((len(cardioRaw) - len(cardio_cleaned))/len(cardioRaw)*100):.2f}%)&quot;)&#10;&#10;#%%&#10;# Final dataset summary with ap_hi as target&#10;print(&quot;FINAL DATASET SUMMARY&quot;)&#10;print(&quot;=&quot;*40)&#10;print(f&quot;Target Variable: ap_hi (Systolic Blood Pressure)&quot;)&#10;print(f&quot;Dataset size: {len(cardio_cleaned)} samples&quot;)&#10;print(f&quot;Features: {len(cardio_cleaned.columns) - 1}&quot;)&#10;print(f&quot;Target range: {cardio_cleaned['ap_hi'].min()} - {cardio_cleaned['ap_hi'].max()}&quot;)&#10;print(f&quot;Target mean: {cardio_cleaned['ap_hi'].mean():.2f}&quot;)&#10;print(f&quot;Target std: {cardio_cleaned['ap_hi'].std():.2f}&quot;)&#10;&#10;# Display final cleaned dataset&#10;cardio_cleaned.head()&#10;&#10;#%%&#10;# STEP 1: Delete null values and prepare data with ap_hi as target&#10;print(&quot;STEP 1: Data Cleaning and Target Selection&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Use the cleaned dataset from previous analysis&#10;target_data = cardio_cleaned.copy()&#10;&#10;# Check for null values&#10;print(&quot;Null values check:&quot;)&#10;print(target_data.isnull().sum())&#10;&#10;# Remove any remaining null values&#10;target_data = target_data.dropna()&#10;print(f&quot;Dataset size after removing nulls: {len(target_data)}&quot;)&#10;&#10;# Set ap_hi as target variable and remove cardio (original binary target)&#10;y = target_data['ap_hi'].copy()&#10;X = target_data.drop(['ap_hi', 'cardio'], axis=1)&#10;&#10;print(f&quot;Target variable (ap_hi) shape: {y.shape}&quot;)&#10;print(f&quot;Features shape: {X.shape}&quot;)&#10;print(f&quot;Feature columns: {X.columns.tolist()}&quot;)&#10;&#10;#%%&#10;# STEP 2: Create interaction features&#10;print(&quot;\nSTEP 2: Feature Engineering - Creating Interaction Features&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Convert age from days to years for better interpretability&#10;X['age_years'] = X['age'] / 365.25&#10;&#10;# Create BMI if not already present&#10;if 'bmi' not in X.columns:&#10;    X['bmi'] = X['weight'] / ((X['height'] / 100) ** 2)&#10;&#10;# Create interaction features that make medical sense&#10;print(&quot;Creating interaction features...&quot;)&#10;&#10;# Age-related interactions&#10;X['age_weight_interaction'] = X['age_years'] * X['weight']&#10;X['age_bmi_interaction'] = X['age_years'] * X['bmi']&#10;&#10;# Blood pressure related interactions&#10;X['blood_pressure_diff'] = X['ap_hi'] - X['ap_lo'] if 'ap_hi' in X.columns else X['ap_lo']  # This will be ap_lo since we removed ap_hi&#10;X['pulse_pressure'] = X['blood_pressure_diff']  # Rename for clarity&#10;&#10;# Health risk interactions&#10;X['cholesterol_glucose_interaction'] = X['cholesterol'] * X['gluc']&#10;X['lifestyle_risk'] = X['smoke'] + X['alco'] - X['active']  # Higher = more risk&#10;&#10;# Physical characteristics&#10;X['height_weight_ratio'] = X['height'] / X['weight']&#10;X['weight_height_interaction'] = X['weight'] * X['height']&#10;&#10;# Age and lifestyle interactions&#10;X['age_smoke_interaction'] = X['age_years'] * X['smoke']&#10;X['age_alcohol_interaction'] = X['age_years'] * X['alco']&#10;&#10;print(f&quot;Features after interaction creation: {X.shape[1]}&quot;)&#10;print(&quot;New interaction features created:&quot;)&#10;interaction_features = ['age_years', 'bmi', 'age_weight_interaction', 'age_bmi_interaction', &#10;                       'pulse_pressure', 'cholesterol_glucose_interaction', 'lifestyle_risk',&#10;                       'height_weight_ratio', 'weight_height_interaction', 'age_smoke_interaction',&#10;                       'age_alcohol_interaction']&#10;for feat in interaction_features:&#10;    if feat in X.columns:&#10;        print(f&quot;  - {feat}&quot;)&#10;&#10;#%%&#10;# STEP 3: Identify and prepare categorical/binary features for encoding&#10;print(&quot;\nSTEP 3: Identifying Features for Encoding&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Identify binary and categorical columns&#10;binary_columns = ['gender', 'smoke', 'alco', 'active']&#10;categorical_columns = ['cholesterol', 'gluc']  # These are ordinal but we'll treat as categorical&#10;numerical_columns = [col for col in X.columns if col not in binary_columns + categorical_columns]&#10;&#10;print(f&quot;Binary columns for one-hot encoding: {binary_columns}&quot;)&#10;print(f&quot;Categorical columns for one-hot encoding: {categorical_columns}&quot;)&#10;print(f&quot;Numerical columns: {numerical_columns}&quot;)&#10;&#10;# Check unique values in categorical columns&#10;for col in categorical_columns:&#10;    print(f&quot;{col} unique values: {sorted(X[col].unique())}&quot;)&#10;&#10;#%%&#10;# STEP 4: Create preprocessing pipeline and fit_transform&#10;print(&quot;\nSTEP 4: Data Preprocessing Pipeline&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;from sklearn.preprocessing import StandardScaler, OneHotEncoder&#10;from sklearn.compose import ColumnTransformer&#10;&#10;# Create preprocessing pipeline&#10;preprocessor = ColumnTransformer(&#10;    transformers=[&#10;        ('num', StandardScaler(), numerical_columns),&#10;        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_columns),&#10;        ('bin', OneHotEncoder(drop='first', sparse_output=False), binary_columns)&#10;    ],&#10;    remainder='passthrough'&#10;)&#10;&#10;# Fit and transform the data&#10;print(&quot;Fitting and transforming features...&quot;)&#10;X_transformed = preprocessor.fit_transform(X)&#10;&#10;# Get feature names after transformation&#10;feature_names = []&#10;# Numerical features&#10;feature_names.extend(numerical_columns)&#10;# Categorical features&#10;for i, col in enumerate(categorical_columns):&#10;    encoder = preprocessor.named_transformers_['cat']&#10;    if hasattr(encoder, 'categories_'):&#10;        categories = encoder.categories_[i]&#10;        feature_names.extend([f&quot;{col}_{cat}&quot; for cat in categories[1:]])  # Skip first due to drop='first'&#10;# Binary features&#10;for i, col in enumerate(binary_columns):&#10;    encoder = preprocessor.named_transformers_['bin']&#10;    if hasattr(encoder, 'categories_'):&#10;        categories = encoder.categories_[i]&#10;        feature_names.extend([f&quot;{col}_{cat}&quot; for cat in categories[1:]])  # Skip first due to drop='first'&#10;&#10;print(f&quot;Final feature count after preprocessing: {X_transformed.shape[1]}&quot;)&#10;print(f&quot;Final dataset shape: {X_transformed.shape}&quot;)&#10;&#10;#%%&#10;# STEP 5: Additional preprocessing for XGBoost&#10;print(&quot;\nSTEP 5: Additional Preprocessing for XGBoost&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# XGBoost specific considerations&#10;print(&quot;XGBoost preprocessing considerations:&quot;)&#10;print(&quot;✓ Handle missing values: Already done&quot;)&#10;print(&quot;✓ Feature scaling: Applied StandardScaler to numerical features&quot;)&#10;print(&quot;✓ Categorical encoding: Applied OneHotEncoder&quot;)&#10;print(&quot;✓ Feature interactions: Created relevant medical interactions&quot;)&#10;&#10;# Check for any infinite or extremely large values&#10;import numpy as np&#10;inf_mask = np.isinf(X_transformed).any(axis=1)&#10;if inf_mask.sum() &gt; 0:&#10;    print(f&quot;Removing {inf_mask.sum()} rows with infinite values&quot;)&#10;    X_transformed = X_transformed[~inf_mask]&#10;    y = y[~inf_mask]&#10;&#10;# Check target variable distribution&#10;print(f&quot;\nTarget variable (ap_hi) statistics:&quot;)&#10;print(f&quot;Mean: {y.mean():.2f}&quot;)&#10;print(f&quot;Std: {y.std():.2f}&quot;)&#10;print(f&quot;Min: {y.min()}&quot;)&#10;print(f&quot;Max: {y.max()}&quot;)&#10;print(f&quot;Range: {y.max() - y.min()}&quot;)&#10;&#10;# Check for outliers in target variable (optional log transformation consideration)&#10;Q1 = y.quantile(0.25)&#10;Q3 = y.quantile(0.75)&#10;IQR = Q3 - Q1&#10;outliers = ((y &lt; (Q1 - 1.5 * IQR)) | (y &gt; (Q3 + 1.5 * IQR))).sum()&#10;print(f&quot;Target outliers (IQR method): {outliers} ({outliers/len(y)*100:.2f}%)&quot;)&#10;&#10;#%%&#10;# STEP 6: Train-Test Split and Save as Parquet&#10;print(&quot;\nSTEP 6: Train-Test Split and Saving Data&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;from sklearn.model_selection import train_test_split&#10;&#10;# Split into 90% train, 10% test&#10;X_train, X_test, y_train, y_test = train_test_split(&#10;    X_transformed, y, &#10;    test_size=0.1, &#10;    random_state=42, &#10;    stratify=None  # Can't stratify continuous target&#10;)&#10;&#10;print(f&quot;Training set size: {X_train.shape[0]} samples&quot;)&#10;print(f&quot;Test set size: {X_test.shape[0]} samples&quot;)&#10;print(f&quot;Training set percentage: {X_train.shape[0]/len(X_transformed)*100:.1f}%&quot;)&#10;print(f&quot;Test set percentage: {X_test.shape[0]/len(X_transformed)*100:.1f}%&quot;)&#10;&#10;# Convert to DataFrames for easier saving&#10;train_features_df = pd.DataFrame(X_train, columns=[f'feature_{i}' for i in range(X_train.shape[1])])&#10;test_features_df = pd.DataFrame(X_test, columns=[f'feature_{i}' for i in range(X_test.shape[1])])&#10;train_target_df = pd.DataFrame(y_train.values, columns=['ap_hi'])&#10;test_target_df = pd.DataFrame(y_test.values, columns=['ap_hi'])&#10;&#10;# Save as parquet files&#10;print(&quot;\nSaving preprocessed data as parquet files...&quot;)&#10;train_features_df.to_parquet('X_train.parquet', index=False)&#10;test_features_df.to_parquet('X_test.parquet', index=False)&#10;train_target_df.to_parquet('y_train.parquet', index=False)&#10;test_target_df.to_parquet('y_test.parquet', index=False)&#10;&#10;print(&quot;✓ X_train.parquet saved&quot;)&#10;print(&quot;✓ X_test.parquet saved&quot;) &#10;print(&quot;✓ y_train.parquet saved&quot;)&#10;print(&quot;✓ y_test.parquet saved&quot;)&#10;&#10;#%%&#10;# STEP 7: Summary and Feature Importance Preparation&#10;print(&quot;\nSTEP 7: Final Summary and Feature Information&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Save feature names and preprocessing info for future reference&#10;feature_info = {&#10;    'feature_names': feature_names if len(feature_names) == X_transformed.shape[1] else [f'feature_{i}' for i in range(X_transformed.shape[1])],&#10;    'numerical_features': numerical_columns,&#10;    'categorical_features': categorical_columns,&#10;    'binary_features': binary_columns,&#10;    'interaction_features': interaction_features,&#10;    'target_variable': 'ap_hi',&#10;    'train_size': X_train.shape[0],&#10;    'test_size': X_test.shape[0],&#10;    'total_features': X_transformed.shape[1]&#10;}&#10;&#10;# Save feature info&#10;import pickle&#10;with open('feature_info.pkl', 'wb') as f:&#10;    pickle.dump(feature_info, f)&#10;&#10;print(&quot;PREPROCESSING COMPLETE!&quot;)&#10;print(&quot;=&quot;*50)&#10;print(f&quot;✓ Target variable: ap_hi (Systolic Blood Pressure)&quot;)&#10;print(f&quot;✓ Dataset cleaned and preprocessed&quot;)&#10;print(f&quot;✓ {len(interaction_features)} interaction features created&quot;)&#10;print(f&quot;✓ Categorical and binary features one-hot encoded&quot;)&#10;print(f&quot;✓ Numerical features standardized&quot;)&#10;print(f&quot;✓ Train-test split: 90%-10%&quot;)&#10;print(f&quot;✓ Data saved as parquet files&quot;)&#10;print(f&quot;✓ Ready for XGBoost regression!&quot;)&#10;&#10;print(f&quot;\nFinal dataset statistics:&quot;)&#10;print(f&quot;  - Training samples: {X_train.shape[0]}&quot;)&#10;print(f&quot;  - Test samples: {X_test.shape[0]}&quot;)&#10;print(f&quot;  - Total features: {X_transformed.shape[1]}&quot;)&#10;print(f&quot;  - Target range: {y.min():.0f} - {y.max():.0f} mmHg&quot;)&#10;print(f&quot;  - Target mean: {y.mean():.1f} ± {y.std():.1f} mmHg&quot;)&#10;&#10;# Display sample of final processed data&#10;print(f&quot;\nSample of preprocessed training data:&quot;)&#10;sample_df = pd.DataFrame(X_train[:5], columns=[f'feature_{i}' for i in range(X_train.shape[1])])&#10;sample_df['target_ap_hi'] = y_train.iloc[:5].values&#10;print(sample_df)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/ModelTrainer.ipynb">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/ModelTrainer.ipynb" />
              <option name="originalContent" value="#%%&#10;import pandas as pd&#10;import numpy as np&#10;import matplotlib.pyplot as plt&#10;import seaborn as sns&#10;from sklearn.model_selection import GridSearchCV, RandomizedSearchCV&#10;from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score&#10;import xgboost as xgb&#10;import shap&#10;import pickle&#10;import warnings&#10;warnings.filterwarnings('ignore')&#10;&#10;# Set style for high-quality plots&#10;plt.style.use('seaborn-v0_8')&#10;plt.rcParams['figure.dpi'] = 300&#10;plt.rcParams['savefig.dpi'] = 300&#10;plt.rcParams['font.size'] = 12&#10;&#10;print(&quot;XGBoost Regression Model Training&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;#%%&#10;# Load the preprocessed data&#10;print(&quot;Loading preprocessed data...&quot;)&#10;&#10;X_train = pd.read_parquet('X_train.parquet')&#10;X_test = pd.read_parquet('X_test.parquet')&#10;y_train = pd.read_parquet('y_train.parquet')['ap_hi']&#10;y_test = pd.read_parquet('y_test.parquet')['ap_hi']&#10;&#10;# Load feature information&#10;with open('feature_info.pkl', 'rb') as f:&#10;    feature_info = pickle.load(f)&#10;&#10;print(f&quot;Training set: {X_train.shape}&quot;)&#10;print(f&quot;Test set: {X_test.shape}&quot;)&#10;print(f&quot;Target variable: {feature_info['target_variable']}&quot;)&#10;print(f&quot;Total features: {feature_info['total_features']}&quot;)&#10;&#10;# Display basic statistics&#10;print(f&quot;\nTarget statistics:&quot;)&#10;print(f&quot;Train - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}&quot;)&#10;print(f&quot;Test  - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}&quot;)&#10;&#10;#%%&#10;# STEP 1: Hyperparameter Tuning&#10;print(&quot;\nSTEP 1: Hyperparameter Tuning&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Define parameter grid for RandomizedSearchCV (more efficient than GridSearch)&#10;param_distributions = {&#10;    'n_estimators': [100, 200, 300, 500],&#10;    'max_depth': [3, 4, 5, 6, 7, 8],&#10;    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],&#10;    'subsample': [0.8, 0.9, 1.0],&#10;    'colsample_bytree': [0.8, 0.9, 1.0],&#10;    'reg_alpha': [0, 0.1, 0.5, 1.0],&#10;    'reg_lambda': [1, 1.5, 2.0, 2.5],&#10;    'min_child_weight': [1, 3, 5]&#10;}&#10;&#10;# Initialize XGBoost regressor&#10;xgb_model = xgb.XGBRegressor(&#10;    objective='reg:squarederror',&#10;    random_state=42,&#10;    n_jobs=-1&#10;)&#10;&#10;# Perform randomized search&#10;print(&quot;Performing hyperparameter tuning...&quot;)&#10;print(&quot;This may take several minutes...&quot;)&#10;&#10;random_search = RandomizedSearchCV(&#10;    estimator=xgb_model,&#10;    param_distributions=param_distributions,&#10;    n_iter=50,  # Number of parameter combinations to try&#10;    cv=5,&#10;    scoring='neg_mean_squared_error',&#10;    n_jobs=-1,&#10;    random_state=42,&#10;    verbose=1&#10;)&#10;&#10;# Fit the random search&#10;random_search.fit(X_train, y_train)&#10;&#10;# Get best parameters&#10;best_params = random_search.best_params_&#10;print(f&quot;\nBest parameters found:&quot;)&#10;for param, value in best_params.items():&#10;    print(f&quot;  {param}: {value}&quot;)&#10;print(f&quot;Best CV score (negative MSE): {random_search.best_score_:.4f}&quot;)&#10;print(f&quot;Best CV RMSE: {np.sqrt(-random_search.best_score_):.4f}&quot;)&#10;&#10;#%%&#10;# STEP 2: Train model with best parameters on full dataset&#10;print(&quot;\nSTEP 2: Training Final Model&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Combine train and test sets for final training&#10;X_full = pd.concat([X_train, X_test], axis=0)&#10;y_full = pd.concat([y_train, y_test], axis=0)&#10;&#10;print(f&quot;Full dataset size: {X_full.shape[0]} samples&quot;)&#10;&#10;# Create final model with best parameters&#10;final_model = xgb.XGBRegressor(&#10;    **best_params,&#10;    objective='reg:squarederror',&#10;    random_state=42,&#10;    n_jobs=-1&#10;)&#10;&#10;# Train on full dataset&#10;print(&quot;Training final model on full dataset...&quot;)&#10;final_model.fit(X_full, y_full)&#10;&#10;# Also keep a model trained only on training set for comparison&#10;train_model = xgb.XGBRegressor(&#10;    **best_params,&#10;    objective='reg:squarederror',&#10;    random_state=42,&#10;    n_jobs=-1&#10;)&#10;train_model.fit(X_train, y_train)&#10;&#10;print(&quot;Model training complete!&quot;)&#10;&#10;#%%&#10;# STEP 3: Generate predictions and calculate metrics&#10;print(&quot;\nSTEP 3: Model Evaluation and Metrics&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Predictions from train-only model&#10;y_train_pred = train_model.predict(X_train)&#10;y_test_pred = train_model.predict(X_test)&#10;&#10;# Predictions from full-data model&#10;y_full_pred = final_model.predict(X_full)&#10;&#10;# Calculate metrics function&#10;def calculate_metrics(y_true, y_pred, set_name):&#10;    mae = mean_absolute_error(y_true, y_pred)&#10;    mse = mean_squared_error(y_true, y_pred)&#10;    rmse = np.sqrt(mse)&#10;    r2 = r2_score(y_true, y_pred)&#10;&#10;    # Standard error (standard deviation of residuals)&#10;    residuals = y_true - y_pred&#10;    std_error = np.std(residuals)&#10;&#10;    return {&#10;        'Dataset': set_name,&#10;        'Mean_Absolute_Error': mae,&#10;        'Standard_Error': std_error,&#10;        'RMSE': rmse,&#10;        'R2_Score': r2,&#10;        'MSE': mse&#10;    }&#10;&#10;# Calculate metrics for all sets&#10;metrics_list = []&#10;metrics_list.append(calculate_metrics(y_train, y_train_pred, 'Train'))&#10;metrics_list.append(calculate_metrics(y_test, y_test_pred, 'Test'))&#10;metrics_list.append(calculate_metrics(y_full, y_full_pred, 'Full_Dataset'))&#10;&#10;# Create metrics dataframe&#10;metrics_df = pd.DataFrame(metrics_list)&#10;metrics_df = metrics_df.round(4)&#10;&#10;print(&quot;Model Performance Metrics:&quot;)&#10;print(&quot;=&quot;*60)&#10;print(metrics_df.to_string(index=False))&#10;&#10;# Save metrics&#10;metrics_df.to_csv('model_metrics.csv', index=False)&#10;print(&quot;\n✓ Metrics saved to model_metrics.csv&quot;)&#10;&#10;#%%&#10;# STEP 4: Plot True vs Predicted values&#10;print(&quot;\nSTEP 4: Visualization - True vs Predicted&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Create subplots for train and test predictions&#10;fig, axes = plt.subplots(1, 2, figsize=(15, 6))&#10;&#10;# Train set plot&#10;axes[0].scatter(y_train, y_train_pred, alpha=0.6, color='blue', s=20)&#10;axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)&#10;axes[0].set_xlabel('True Systolic BP (mmHg)')&#10;axes[0].set_ylabel('Predicted Systolic BP (mmHg)')&#10;axes[0].set_title(f'Training Set\nR² = {metrics_df.loc[0, &quot;R2_Score&quot;]:.4f}, RMSE = {metrics_df.loc[0, &quot;RMSE&quot;]:.2f}')&#10;axes[0].grid(True, alpha=0.3)&#10;&#10;# Test set plot&#10;axes[1].scatter(y_test, y_test_pred, alpha=0.6, color='green', s=20)&#10;axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)&#10;axes[1].set_xlabel('True Systolic BP (mmHg)')&#10;axes[1].set_ylabel('Predicted Systolic BP (mmHg)')&#10;axes[1].set_title(f'Test Set\nR² = {metrics_df.loc[1, &quot;R2_Score&quot;]:.4f}, RMSE = {metrics_df.loc[1, &quot;RMSE&quot;]:.2f}')&#10;axes[1].grid(True, alpha=0.3)&#10;&#10;plt.tight_layout()&#10;plt.savefig('true_vs_predicted.svg', format='svg', dpi=300, bbox_inches='tight')&#10;plt.savefig('true_vs_predicted.png', format='png', dpi=300, bbox_inches='tight')&#10;plt.show()&#10;&#10;print(&quot;✓ True vs Predicted plots saved as vector (SVG) and PNG&quot;)&#10;&#10;#%%&#10;# Additional residual plots&#10;fig, axes = plt.subplots(1, 2, figsize=(15, 6))&#10;&#10;# Residual plot for train&#10;train_residuals = y_train - y_train_pred&#10;axes[0].scatter(y_train_pred, train_residuals, alpha=0.6, color='blue', s=20)&#10;axes[0].axhline(y=0, color='r', linestyle='--')&#10;axes[0].set_xlabel('Predicted Systolic BP (mmHg)')&#10;axes[0].set_ylabel('Residuals (mmHg)')&#10;axes[0].set_title('Training Set Residuals')&#10;axes[0].grid(True, alpha=0.3)&#10;&#10;# Residual plot for test&#10;test_residuals = y_test - y_test_pred&#10;axes[1].scatter(y_test_pred, test_residuals, alpha=0.6, color='green', s=20)&#10;axes[1].axhline(y=0, color='r', linestyle='--')&#10;axes[1].set_xlabel('Predicted Systolic BP (mmHg)')&#10;axes[1].set_ylabel('Residuals (mmHg)')&#10;axes[1].set_title('Test Set Residuals')&#10;axes[1].grid(True, alpha=0.3)&#10;&#10;plt.tight_layout()&#10;plt.savefig('residual_plots.svg', format='svg', dpi=300, bbox_inches='tight')&#10;plt.savefig('residual_plots.png', format='png', dpi=300, bbox_inches='tight')&#10;plt.show()&#10;&#10;print(&quot;✓ Residual plots saved as vector (SVG) and PNG&quot;)&#10;&#10;#%%&#10;# STEP 5: SHAP Analysis with actual feature names&#10;print(&quot;\nSTEP 5: SHAP Analysis and Feature Importance&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Create feature names mapping&#10;def create_feature_names():&#10;    &quot;&quot;&quot;Create meaningful feature names from the preprocessing info&quot;&quot;&quot;&#10;    feature_names = []&#10;&#10;    # Add numerical features&#10;    numerical_features = feature_info['numerical_features']&#10;    for feat in numerical_features:&#10;        if feat in ['age', 'height', 'weight', 'ap_lo']:&#10;            feature_names.append(feat)&#10;        elif 'age_years' in feat:&#10;            feature_names.append('Age (years)')&#10;        elif 'bmi' in feat:&#10;            feature_names.append('BMI')&#10;        elif 'interaction' in feat:&#10;            feature_names.append(feat.replace('_', ' ').title())&#10;        else:&#10;            feature_names.append(feat.replace('_', ' ').title())&#10;&#10;    # Add categorical features (cholesterol, glucose)&#10;    cat_features = ['cholesterol', 'gluc']&#10;    for cat in cat_features:&#10;        if cat == 'cholesterol':&#10;            feature_names.extend(['Cholesterol_High', 'Cholesterol_Very_High'])&#10;        elif cat == 'gluc':&#10;            feature_names.extend(['Glucose_High', 'Glucose_Very_High'])&#10;&#10;    # Add binary features&#10;    binary_features = ['gender', 'smoke', 'alco', 'active']&#10;    binary_names = ['Gender_Male', 'Smoking', 'Alcohol', 'Physical_Activity']&#10;    feature_names.extend(binary_names)&#10;&#10;    return feature_names&#10;&#10;# Get feature names&#10;try:&#10;    if len(feature_info['feature_names']) == X_train.shape[1]:&#10;        feature_names = feature_info['feature_names']&#10;    else:&#10;        feature_names = create_feature_names()&#10;except:&#10;    feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]&#10;&#10;# Ensure we have the right number of feature names&#10;if len(feature_names) != X_train.shape[1]:&#10;    feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]&#10;&#10;print(f&quot;Using {len(feature_names)} feature names for SHAP analysis&quot;)&#10;&#10;# Initialize SHAP explainer&#10;print(&quot;Initializing SHAP explainer...&quot;)&#10;explainer = shap.Explainer(train_model)&#10;&#10;# Calculate SHAP values (use a sample for speed)&#10;sample_size = min(1000, len(X_test))&#10;X_sample = X_test.iloc[:sample_size]&#10;print(f&quot;Calculating SHAP values for {sample_size} samples...&quot;)&#10;&#10;shap_values = explainer(X_sample)&#10;&#10;#%%&#10;# SHAP Summary Plot&#10;print(&quot;Creating SHAP summary plot...&quot;)&#10;plt.figure(figsize=(12, 8))&#10;shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False)&#10;plt.title('SHAP Summary Plot - Feature Importance for Systolic Blood Pressure Prediction', fontsize=14, pad=20)&#10;plt.tight_layout()&#10;plt.savefig('shap_summary_plot.svg', format='svg', dpi=300, bbox_inches='tight')&#10;plt.savefig('shap_summary_plot.png', format='png', dpi=300, bbox_inches='tight')&#10;plt.show()&#10;&#10;print(&quot;✓ SHAP summary plot saved as vector (SVG) and PNG&quot;)&#10;&#10;#%%&#10;# SHAP Dependency Plots for top features&#10;print(&quot;Creating SHAP dependency plots...&quot;)&#10;&#10;# Get feature importance ranking&#10;feature_importance = np.abs(shap_values.values).mean(0)&#10;top_features_idx = np.argsort(feature_importance)[-6:][::-1]  # Top 6 features&#10;&#10;fig, axes = plt.subplots(2, 3, figsize=(18, 12))&#10;axes = axes.ravel()&#10;&#10;for i, feature_idx in enumerate(top_features_idx):&#10;    plt.sca(axes[i])&#10;    shap.dependence_plot(&#10;        feature_idx,&#10;        shap_values.values,&#10;        X_sample,&#10;        feature_names=feature_names,&#10;        show=False,&#10;        ax=axes[i]&#10;    )&#10;    axes[i].set_title(f'SHAP Dependency: {feature_names[feature_idx]}', fontsize=12)&#10;&#10;plt.suptitle('SHAP Dependency Plots - Top 6 Features', fontsize=16, y=0.98)&#10;plt.tight_layout()&#10;plt.savefig('shap_dependency_plots.svg', format='svg', dpi=300, bbox_inches='tight')&#10;plt.savefig('shap_dependency_plots.png', format='png', dpi=300, bbox_inches='tight')&#10;plt.show()&#10;&#10;print(&quot;✓ SHAP dependency plots saved as vector (SVG) and PNG&quot;)&#10;&#10;#%%&#10;# Feature Importance Bar Plot with actual feature names&#10;plt.figure(figsize=(12, 8))&#10;&#10;# Create a copy of shap_values with proper feature names&#10;shap_values_copy = shap_values.copy()&#10;shap_values_copy.feature_names = feature_names&#10;&#10;shap.plots.bar(shap_values_copy, max_display=15, show=False)&#10;plt.title('SHAP Feature Importance - Top 15 Features', fontsize=14, pad=20)&#10;plt.tight_layout()&#10;plt.savefig('shap_feature_importance.svg', format='svg', dpi=300, bbox_inches='tight')&#10;plt.savefig('shap_feature_importance.png', format='png', dpi=300, bbox_inches='tight')&#10;plt.show()&#10;&#10;print(&quot;✓ SHAP feature importance plot saved as vector (SVG) and PNG&quot;)&#10;&#10;#%%&#10;# STEP 6: Save Best Hyperparameters and Model Summary&#10;print(&quot;\nSTEP 6: Save Best Hyperparameters and Final Model Summary&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Save best hyperparameters&#10;hyperparams_df = pd.DataFrame([best_params]).T&#10;hyperparams_df.columns = ['Best_Value']&#10;hyperparams_df.index.name = 'Parameter'&#10;hyperparams_df.to_csv('best_hyperparameters.csv')&#10;&#10;print(&quot;Best Hyperparameters:&quot;)&#10;print(&quot;=&quot;*30)&#10;for param, value in best_params.items():&#10;    print(f&quot;{param}: {value}&quot;)&#10;&#10;print(f&quot;\nBest CV RMSE: {np.sqrt(-random_search.best_score_):.4f}&quot;)&#10;print(&quot;✓ Best hyperparameters saved to best_hyperparameters.csv&quot;)&#10;&#10;# Save additional model info&#10;model_info = {&#10;    'best_params': best_params,&#10;    'best_cv_score': random_search.best_score_,&#10;    'best_cv_rmse': np.sqrt(-random_search.best_score_),&#10;    'feature_names': feature_names,&#10;    'model_type': 'XGBoost Regressor',&#10;    'target_variable': 'ap_hi (Systolic Blood Pressure)',&#10;    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')&#10;}&#10;&#10;with open('model_info.pkl', 'wb') as f:&#10;    pickle.dump(model_info, f)&#10;&#10;print(&quot;✓ Complete model information saved to model_info.pkl&quot;)&#10;&#10;# Save the trained model&#10;final_model.save_model('xgboost_final_model.json')&#10;train_model.save_model('xgboost_train_model.json')&#10;&#10;print(&quot;MODEL TRAINING COMPLETE!&quot;)&#10;print(&quot;=&quot;*60)&#10;print(f&quot;✓ Best hyperparameters found and applied&quot;)&#10;print(f&quot;✓ Final model trained on full dataset ({X_full.shape[0]} samples)&quot;)&#10;print(f&quot;✓ Model performance metrics calculated and saved&quot;)&#10;print(f&quot;✓ Visualizations created and saved as high-DPI vector graphics&quot;)&#10;print(f&quot;✓ SHAP analysis completed with actual feature names&quot;)&#10;print(f&quot;✓ Models saved as JSON files&quot;)&#10;&#10;print(f&quot;\nFinal Model Performance Summary:&quot;)&#10;print(f&quot;  Training Set - RMSE: {metrics_df.loc[0, 'RMSE']:.2f} mmHg, R²: {metrics_df.loc[0, 'R2_Score']:.4f}&quot;)&#10;print(f&quot;  Test Set     - RMSE: {metrics_df.loc[1, 'RMSE']:.2f} mmHg, R²: {metrics_df.loc[1, 'R2_Score']:.4f}&quot;)&#10;print(f&quot;  Full Dataset - RMSE: {metrics_df.loc[2, 'RMSE']:.2f} mmHg, R²: {metrics_df.loc[2, 'R2_Score']:.4f}&quot;)&#10;&#10;print(f&quot;\nFiles Generated:&quot;)&#10;print(f&quot;   model_metrics.csv - Performance metrics&quot;)&#10;print(f&quot;   true_vs_predicted.svg/png - Prediction accuracy plots&quot;)&#10;print(f&quot;   residual_plots.svg/png - Residual analysis&quot;)&#10;print(f&quot;   shap_summary_plot.svg/png - SHAP feature importance&quot;)&#10;print(f&quot;   shap_dependency_plots.svg/png - SHAP dependency analysis&quot;)&#10;print(f&quot;   shap_feature_importance.svg/png - Feature importance ranking&quot;)&#10;print(f&quot;   xgboost_final_model.json - Trained model (full data)&quot;)&#10;print(f&quot;   xgboost_train_model.json - Trained model (train only)&quot;)&#10;&#10;print(f&quot;\nModel is ready for deployment and can predict systolic blood pressure!&quot;)&#10;" />
              <option name="updatedContent" value="#%%&#10;import pandas as pd&#10;import numpy as np&#10;import matplotlib.pyplot as plt&#10;import seaborn as sns&#10;from sklearn.model_selection import GridSearchCV, RandomizedSearchCV&#10;from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score&#10;import xgboost as xgb&#10;import shap&#10;import pickle&#10;import warnings&#10;warnings.filterwarnings('ignore')&#10;&#10;# Set style for high-quality plots&#10;plt.style.use('seaborn-v0_8')&#10;plt.rcParams['figure.dpi'] = 300&#10;plt.rcParams['savefig.dpi'] = 300&#10;plt.rcParams['font.size'] = 12&#10;&#10;print(&quot;XGBoost Regression Model Training&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;#%%&#10;# Load the preprocessed data&#10;print(&quot;Loading preprocessed data...&quot;)&#10;&#10;X_train = pd.read_parquet('X_train.parquet')&#10;X_test = pd.read_parquet('X_test.parquet')&#10;y_train = pd.read_parquet('y_train.parquet')['ap_hi']&#10;y_test = pd.read_parquet('y_test.parquet')['ap_hi']&#10;&#10;# Load feature information&#10;with open('feature_info.pkl', 'rb') as f:&#10;    feature_info = pickle.load(f)&#10;&#10;print(f&quot;Training set: {X_train.shape}&quot;)&#10;print(f&quot;Test set: {X_test.shape}&quot;)&#10;print(f&quot;Target variable: {feature_info['target_variable']}&quot;)&#10;print(f&quot;Total features: {feature_info['total_features']}&quot;)&#10;&#10;# Display basic statistics&#10;print(f&quot;\nTarget statistics:&quot;)&#10;print(f&quot;Train - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}&quot;)&#10;print(f&quot;Test  - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}&quot;)&#10;&#10;#%%&#10;# STEP 1: Hyperparameter Tuning&#10;print(&quot;\nSTEP 1: Hyperparameter Tuning&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Define parameter grid for RandomizedSearchCV (more efficient than GridSearch)&#10;param_distributions = {&#10;    'n_estimators': [100, 200, 300, 500],&#10;    'max_depth': [3, 4, 5, 6, 7, 8],&#10;    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],&#10;    'subsample': [0.8, 0.9, 1.0],&#10;    'colsample_bytree': [0.8, 0.9, 1.0],&#10;    'reg_alpha': [0, 0.1, 0.5, 1.0],&#10;    'reg_lambda': [1, 1.5, 2.0, 2.5],&#10;    'min_child_weight': [1, 3, 5]&#10;}&#10;&#10;# Initialize XGBoost regressor&#10;xgb_model = xgb.XGBRegressor(&#10;    objective='reg:squarederror',&#10;    random_state=42,&#10;    n_jobs=-1&#10;)&#10;&#10;# Perform randomized search&#10;print(&quot;Performing hyperparameter tuning...&quot;)&#10;print(&quot;This may take several minutes...&quot;)&#10;&#10;random_search = RandomizedSearchCV(&#10;    estimator=xgb_model,&#10;    param_distributions=param_distributions,&#10;    n_iter=50,  # Number of parameter combinations to try&#10;    cv=5,&#10;    scoring='neg_mean_squared_error',&#10;    n_jobs=-1,&#10;    random_state=42,&#10;    verbose=1&#10;)&#10;&#10;# Fit the random search&#10;random_search.fit(X_train, y_train)&#10;&#10;# Get best parameters&#10;best_params = random_search.best_params_&#10;print(f&quot;\nBest parameters found:&quot;)&#10;for param, value in best_params.items():&#10;    print(f&quot;  {param}: {value}&quot;)&#10;print(f&quot;Best CV score (negative MSE): {random_search.best_score_:.4f}&quot;)&#10;print(f&quot;Best CV RMSE: {np.sqrt(-random_search.best_score_):.4f}&quot;)&#10;&#10;#%%&#10;# STEP 2: Train model with best parameters on full dataset&#10;print(&quot;\nSTEP 2: Training Final Model&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Combine train and test sets for final training&#10;X_full = pd.concat([X_train, X_test], axis=0)&#10;y_full = pd.concat([y_train, y_test], axis=0)&#10;&#10;print(f&quot;Full dataset size: {X_full.shape[0]} samples&quot;)&#10;&#10;# Create final model with best parameters&#10;final_model = xgb.XGBRegressor(&#10;    **best_params,&#10;    objective='reg:squarederror',&#10;    random_state=42,&#10;    n_jobs=-1&#10;)&#10;&#10;# Train on full dataset&#10;print(&quot;Training final model on full dataset...&quot;)&#10;final_model.fit(X_full, y_full)&#10;&#10;# Also keep a model trained only on training set for comparison&#10;train_model = xgb.XGBRegressor(&#10;    **best_params,&#10;    objective='reg:squarederror',&#10;    random_state=42,&#10;    n_jobs=-1&#10;)&#10;train_model.fit(X_train, y_train)&#10;&#10;print(&quot;Model training complete!&quot;)&#10;&#10;#%%&#10;# STEP 3: Generate predictions and calculate metrics&#10;print(&quot;\nSTEP 3: Model Evaluation and Metrics&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Predictions from train-only model&#10;y_train_pred = train_model.predict(X_train)&#10;y_test_pred = train_model.predict(X_test)&#10;&#10;# Predictions from full-data model&#10;y_full_pred = final_model.predict(X_full)&#10;&#10;# Calculate metrics function&#10;def calculate_metrics(y_true, y_pred, set_name):&#10;    mae = mean_absolute_error(y_true, y_pred)&#10;    mse = mean_squared_error(y_true, y_pred)&#10;    rmse = np.sqrt(mse)&#10;    r2 = r2_score(y_true, y_pred)&#10;&#10;    # Standard error (standard deviation of residuals)&#10;    residuals = y_true - y_pred&#10;    std_error = np.std(residuals)&#10;&#10;    return {&#10;        'Dataset': set_name,&#10;        'Mean_Absolute_Error': mae,&#10;        'Standard_Error': std_error,&#10;        'RMSE': rmse,&#10;        'R2_Score': r2,&#10;        'MSE': mse&#10;    }&#10;&#10;# Calculate metrics for all sets&#10;metrics_list = []&#10;metrics_list.append(calculate_metrics(y_train, y_train_pred, 'Train'))&#10;metrics_list.append(calculate_metrics(y_test, y_test_pred, 'Test'))&#10;metrics_list.append(calculate_metrics(y_full, y_full_pred, 'Full_Dataset'))&#10;&#10;# Create metrics dataframe&#10;metrics_df = pd.DataFrame(metrics_list)&#10;metrics_df = metrics_df.round(4)&#10;&#10;print(&quot;Model Performance Metrics:&quot;)&#10;print(&quot;=&quot;*60)&#10;print(metrics_df.to_string(index=False))&#10;&#10;# Save metrics&#10;metrics_df.to_csv('model_metrics.csv', index=False)&#10;print(&quot;\n✓ Metrics saved to model_metrics.csv&quot;)&#10;&#10;#%%&#10;# STEP 4: Plot True vs Predicted values&#10;print(&quot;\nSTEP 4: Visualization - True vs Predicted&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Create subplots for train and test predictions&#10;fig, axes = plt.subplots(1, 2, figsize=(15, 6))&#10;&#10;# Train set plot&#10;axes[0].scatter(y_train, y_train_pred, alpha=0.6, color='blue', s=20)&#10;axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)&#10;axes[0].set_xlabel('True Systolic BP (mmHg)')&#10;axes[0].set_ylabel('Predicted Systolic BP (mmHg)')&#10;axes[0].set_title(f'Training Set\nR² = {metrics_df.loc[0, &quot;R2_Score&quot;]:.4f}, RMSE = {metrics_df.loc[0, &quot;RMSE&quot;]:.2f}')&#10;axes[0].grid(True, alpha=0.3)&#10;&#10;# Test set plot&#10;axes[1].scatter(y_test, y_test_pred, alpha=0.6, color='green', s=20)&#10;axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)&#10;axes[1].set_xlabel('True Systolic BP (mmHg)')&#10;axes[1].set_ylabel('Predicted Systolic BP (mmHg)')&#10;axes[1].set_title(f'Test Set\nR² = {metrics_df.loc[1, &quot;R2_Score&quot;]:.4f}, RMSE = {metrics_df.loc[1, &quot;RMSE&quot;]:.2f}')&#10;axes[1].grid(True, alpha=0.3)&#10;&#10;plt.tight_layout()&#10;plt.savefig('true_vs_predicted.svg', format='svg', dpi=300, bbox_inches='tight')&#10;plt.savefig('true_vs_predicted.png', format='png', dpi=300, bbox_inches='tight')&#10;plt.show()&#10;&#10;print(&quot;✓ True vs Predicted plots saved as vector (SVG) and PNG&quot;)&#10;&#10;#%%&#10;# Additional residual plots&#10;fig, axes = plt.subplots(1, 2, figsize=(15, 6))&#10;&#10;# Residual plot for train&#10;train_residuals = y_train - y_train_pred&#10;axes[0].scatter(y_train_pred, train_residuals, alpha=0.6, color='blue', s=20)&#10;axes[0].axhline(y=0, color='r', linestyle='--')&#10;axes[0].set_xlabel('Predicted Systolic BP (mmHg)')&#10;axes[0].set_ylabel('Residuals (mmHg)')&#10;axes[0].set_title('Training Set Residuals')&#10;axes[0].grid(True, alpha=0.3)&#10;&#10;# Residual plot for test&#10;test_residuals = y_test - y_test_pred&#10;axes[1].scatter(y_test_pred, test_residuals, alpha=0.6, color='green', s=20)&#10;axes[1].axhline(y=0, color='r', linestyle='--')&#10;axes[1].set_xlabel('Predicted Systolic BP (mmHg)')&#10;axes[1].set_ylabel('Residuals (mmHg)')&#10;axes[1].set_title('Test Set Residuals')&#10;axes[1].grid(True, alpha=0.3)&#10;&#10;plt.tight_layout()&#10;plt.savefig('residual_plots.svg', format='svg', dpi=300, bbox_inches='tight')&#10;plt.savefig('residual_plots.png', format='png', dpi=300, bbox_inches='tight')&#10;plt.show()&#10;&#10;print(&quot;✓ Residual plots saved as vector (SVG) and PNG&quot;)&#10;&#10;#%%&#10;# STEP 5: SHAP Analysis with actual feature names&#10;print(&quot;\nSTEP 5: SHAP Analysis and Feature Importance&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Create feature names mapping&#10;def create_feature_names():&#10;    &quot;&quot;&quot;Create meaningful feature names from the preprocessing info&quot;&quot;&quot;&#10;    feature_names = []&#10;&#10;    # Add numerical features&#10;    numerical_features = feature_info['numerical_features']&#10;    for feat in numerical_features:&#10;        if feat in ['age', 'height', 'weight', 'ap_lo']:&#10;            feature_names.append(feat)&#10;        elif 'age_years' in feat:&#10;            feature_names.append('Age (years)')&#10;        elif 'bmi' in feat:&#10;            feature_names.append('BMI')&#10;        elif 'interaction' in feat:&#10;            feature_names.append(feat.replace('_', ' ').title())&#10;        else:&#10;            feature_names.append(feat.replace('_', ' ').title())&#10;&#10;    # Add categorical features (cholesterol, glucose)&#10;    cat_features = ['cholesterol', 'gluc']&#10;    for cat in cat_features:&#10;        if cat == 'cholesterol':&#10;            feature_names.extend(['Cholesterol_High', 'Cholesterol_Very_High'])&#10;        elif cat == 'gluc':&#10;            feature_names.extend(['Glucose_High', 'Glucose_Very_High'])&#10;&#10;    # Add binary features&#10;    binary_features = ['gender', 'smoke', 'alco', 'active']&#10;    binary_names = ['Gender_Male', 'Smoking', 'Alcohol', 'Physical_Activity']&#10;    feature_names.extend(binary_names)&#10;&#10;    return feature_names&#10;&#10;# Get feature names&#10;try:&#10;    if len(feature_info['feature_names']) == X_train.shape[1]:&#10;        feature_names = feature_info['feature_names']&#10;    else:&#10;        feature_names = create_feature_names()&#10;except:&#10;    feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]&#10;&#10;# Ensure we have the right number of feature names&#10;if len(feature_names) != X_train.shape[1]:&#10;    feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]&#10;&#10;print(f&quot;Using {len(feature_names)} feature names for SHAP analysis&quot;)&#10;&#10;# Initialize SHAP explainer&#10;print(&quot;Initializing SHAP explainer...&quot;)&#10;explainer = shap.Explainer(train_model)&#10;&#10;# Calculate SHAP values (use a sample for speed)&#10;sample_size = min(1000, len(X_test))&#10;X_sample = X_test.iloc[:sample_size]&#10;print(f&quot;Calculating SHAP values for {sample_size} samples...&quot;)&#10;&#10;shap_values = explainer(X_sample)&#10;&#10;#%%&#10;# SHAP Summary Plot&#10;print(&quot;Creating SHAP summary plot...&quot;)&#10;plt.figure(figsize=(12, 8))&#10;shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False)&#10;plt.title('SHAP Summary Plot - Feature Importance for Systolic Blood Pressure Prediction', fontsize=14, pad=20)&#10;plt.tight_layout()&#10;plt.savefig('shap_summary_plot.svg', format='svg', dpi=300, bbox_inches='tight')&#10;plt.savefig('shap_summary_plot.png', format='png', dpi=300, bbox_inches='tight')&#10;plt.show()&#10;&#10;print(&quot;✓ SHAP summary plot saved as vector (SVG) and PNG&quot;)&#10;&#10;#%%&#10;# SHAP Dependency Plots for top features&#10;print(&quot;Creating SHAP dependency plots...&quot;)&#10;&#10;# Get feature importance ranking&#10;feature_importance = np.abs(shap_values.values).mean(0)&#10;top_features_idx = np.argsort(feature_importance)[-6:][::-1]  # Top 6 features&#10;&#10;fig, axes = plt.subplots(2, 3, figsize=(18, 12))&#10;axes = axes.ravel()&#10;&#10;for i, feature_idx in enumerate(top_features_idx):&#10;    plt.sca(axes[i])&#10;    shap.dependence_plot(&#10;        feature_idx,&#10;        shap_values.values,&#10;        X_sample,&#10;        feature_names=feature_names,&#10;        show=False,&#10;        ax=axes[i]&#10;    )&#10;    axes[i].set_title(f'SHAP Dependency: {feature_names[feature_idx]}', fontsize=12)&#10;&#10;plt.suptitle('SHAP Dependency Plots - Top 6 Features', fontsize=16, y=0.98)&#10;plt.tight_layout()&#10;plt.savefig('shap_dependency_plots.svg', format='svg', dpi=300, bbox_inches='tight')&#10;plt.savefig('shap_dependency_plots.png', format='png', dpi=300, bbox_inches='tight')&#10;plt.show()&#10;&#10;print(&quot;✓ SHAP dependency plots saved as vector (SVG) and PNG&quot;)&#10;&#10;#%%&#10;# Feature Importance Bar Plot with actual feature names&#10;plt.figure(figsize=(12, 8))&#10;&#10;# Create feature importance manually with proper names&#10;feature_importance_values = np.abs(shap_values.values).mean(0)&#10;sorted_idx = np.argsort(feature_importance_values)[-15:]  # Top 15 features&#10;&#10;# Create DataFrame for plotting&#10;importance_df = pd.DataFrame({&#10;    'feature': [feature_names[i] for i in sorted_idx],&#10;    'importance': feature_importance_values[sorted_idx]&#10;})&#10;&#10;# Create horizontal bar plot&#10;plt.barh(range(len(importance_df)), importance_df['importance'])&#10;plt.yticks(range(len(importance_df)), importance_df['feature'])&#10;plt.xlabel('Mean |SHAP value|')&#10;plt.title('SHAP Feature Importance - Top 15 Features', fontsize=14, pad=20)&#10;plt.gca().invert_yaxis()  # Highest importance at top&#10;plt.tight_layout()&#10;plt.savefig('shap_feature_importance.svg', format='svg', dpi=300, bbox_inches='tight')&#10;plt.savefig('shap_feature_importance.png', format='png', dpi=300, bbox_inches='tight')&#10;plt.show()&#10;&#10;print(&quot;✓ SHAP feature importance plot saved as vector (SVG) and PNG&quot;)&#10;&#10;#%%&#10;# STEP 6: Save Best Hyperparameters and Model Summary&#10;print(&quot;\nSTEP 6: Save Best Hyperparameters and Final Model Summary&quot;)&#10;print(&quot;=&quot;*50)&#10;&#10;# Save best hyperparameters&#10;hyperparams_df = pd.DataFrame([best_params]).T&#10;hyperparams_df.columns = ['Best_Value']&#10;hyperparams_df.index.name = 'Parameter'&#10;hyperparams_df.to_csv('best_hyperparameters.csv')&#10;&#10;print(&quot;Best Hyperparameters:&quot;)&#10;print(&quot;=&quot;*30)&#10;for param, value in best_params.items():&#10;    print(f&quot;{param}: {value}&quot;)&#10;&#10;print(f&quot;\nBest CV RMSE: {np.sqrt(-random_search.best_score_):.4f}&quot;)&#10;print(&quot;✓ Best hyperparameters saved to best_hyperparameters.csv&quot;)&#10;&#10;# Save additional model info&#10;model_info = {&#10;    'best_params': best_params,&#10;    'best_cv_score': random_search.best_score_,&#10;    'best_cv_rmse': np.sqrt(-random_search.best_score_),&#10;    'feature_names': feature_names,&#10;    'model_type': 'XGBoost Regressor',&#10;    'target_variable': 'ap_hi (Systolic Blood Pressure)',&#10;    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')&#10;}&#10;&#10;with open('model_info.pkl', 'wb') as f:&#10;    pickle.dump(model_info, f)&#10;&#10;print(&quot;✓ Complete model information saved to model_info.pkl&quot;)&#10;&#10;# Save the trained model&#10;final_model.save_model('xgboost_final_model.json')&#10;train_model.save_model('xgboost_train_model.json')&#10;&#10;print(&quot;MODEL TRAINING COMPLETE!&quot;)&#10;print(&quot;=&quot;*60)&#10;print(f&quot;✓ Best hyperparameters found and applied&quot;)&#10;print(f&quot;✓ Final model trained on full dataset ({X_full.shape[0]} samples)&quot;)&#10;print(f&quot;✓ Model performance metrics calculated and saved&quot;)&#10;print(f&quot;✓ Visualizations created and saved as high-DPI vector graphics&quot;)&#10;print(f&quot;✓ SHAP analysis completed with actual feature names&quot;)&#10;print(f&quot;✓ Models saved as JSON files&quot;)&#10;&#10;print(f&quot;\nFinal Model Performance Summary:&quot;)&#10;print(f&quot;  Training Set - RMSE: {metrics_df.loc[0, 'RMSE']:.2f} mmHg, R²: {metrics_df.loc[0, 'R2_Score']:.4f}&quot;)&#10;print(f&quot;  Test Set     - RMSE: {metrics_df.loc[1, 'RMSE']:.2f} mmHg, R²: {metrics_df.loc[1, 'R2_Score']:.4f}&quot;)&#10;print(f&quot;  Full Dataset - RMSE: {metrics_df.loc[2, 'RMSE']:.2f} mmHg, R²: {metrics_df.loc[2, 'R2_Score']:.4f}&quot;)&#10;&#10;print(f&quot;\nFiles Generated:&quot;)&#10;print(f&quot;   model_metrics.csv - Performance metrics&quot;)&#10;print(f&quot;   true_vs_predicted.svg/png - Prediction accuracy plots&quot;)&#10;print(f&quot;   residual_plots.svg/png - Residual analysis&quot;)&#10;print(f&quot;   shap_summary_plot.svg/png - SHAP feature importance&quot;)&#10;print(f&quot;   shap_dependency_plots.svg/png - SHAP dependency analysis&quot;)&#10;print(f&quot;   shap_feature_importance.svg/png - Feature importance ranking&quot;)&#10;print(f&quot;   xgboost_final_model.json - Trained model (full data)&quot;)&#10;print(f&quot;   xgboost_train_model.json - Trained model (train only)&quot;)&#10;&#10;print(f&quot;\nModel is ready for deployment and can predict systolic blood pressure!&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/analyze_data.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/analyze_data.py" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>